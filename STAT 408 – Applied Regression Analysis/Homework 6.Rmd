---
title: "Homework 6"
author: "Charles Hwang"
date: "12/11/2022"
output: pdf_document
---
Charles Hwang

Dr. Xi

STAT 408-001

2022 December 11

## Problem 1

```{r Problem 1}
rm(list=ls())
library(datasets); data(mtcars)
```

### Problem 1a

```{r Problem 1a}
summary(glm(am~mpg+hp,family=binomial,data=mtcars))
tm<-summary(glm(am~mpg+hp,family=binomial,data=mtcars))$coefficients
```

We can see that the estimated regression coefficients from this logistic model are $\beta_0$ = `r tm["(Intercept)","Estimate"]`, $\beta_{mpg}$ = `r tm["mpg","Estimate"]`, and $\beta_{hp}$ = `r tm["hp","Estimate"]`.

Interpretation of $\beta_0$ (intercept term): We estimate a hypothetical car with 0 miles-per-gallon and 0 horsepower (which would not make sense) would have a $\frac{e^{-33.60517}}{1+e^{-33.60517}}=$ `r format(100*exp(tm["(Intercept)","Estimate"])/(1+exp(tm["(Intercept)","Estimate"])),scientific=FALSE)` percent probability of having a manual transmission ($P(Y=1)$).

Interpretation of $\beta_{mpg}$ (coefficient for miles per gallon): We estimate there is approximately a $e^{1.259615}-1=$ `r 100*(exp(tm["mpg","Estimate"])-1)` **percent increase** in the odds of having a manual transmission ($P(Y=1)$) for every 1 mile-per-gallon increase in a car's fuel economy, holding all other variables constant.

Interpretation of $\beta_{hp}$ (coefficient for horsepower): We estimate there is approximately a $e^{0.05504458}-1=$ `r 100*(exp(tm["hp","Estimate"])-1)` **percent increase** in the odds of having a manual transmission ($P(Y=1)$) for every 1 horsepower increase in a car's power output, holding all other variables constant.

### Problem 1b

```{r Problem 1b}
1-1/(1+exp(-(tm["(Intercept)","Estimate"]+20*tm["mpg","Estimate"]+180*tm["hp","Estimate"])))
```

We predict there is approximately a $1-\frac{1}{1+e^{-(-33.60517+1.259615(20)+0.05504458(180))}}=$ `r 100*(1-1/(1+exp(-(tm["(Intercept)","Estimate"]+20*tm["mpg","Estimate"]+180*tm["hp","Estimate"]))))` percent probability that a car with a fuel economy of 20 miles per gallon and a power output of 180 horsepower has an *automatic* transmission ($P(Y=0)=1-P(Y=1)$).

### Problem 1c

```{r Problem 1c}
set.seed(1211)
samp<-sample(1:nrow(mtcars),round(0.8*nrow(mtcars)))
train<-mtcars[samp,]
test<-mtcars[-samp,]
t<-table(test$am,round(predict(glm(am~mpg+hp,family=binomial,data=train),test,type="response")))
sum(diag(t))/nrow(test)
```

We can see the prediction accuracy is `r 100*sum(diag(t))/nrow(test)` percent.

### Problem 1d

```{r Problem 1d}
t
Sensitivity<-t["1","1"]/sum(t["1",]) # True positive rate
Specificity<-t["0","0"]/sum(t["0",]) # True negative rate
Precision<-t["1","1"]/sum(t[,"1"])
data.frame(Sensitivity,Specificity,Precision)
```

We can see the sensitivity (true positive rate) is `r Sensitivity`, the specificity (true negative rate) is `r Specificity`, and the precision is `r Precision`.

## Problem 2

```{r Problem 2}
s<-read.csv("/Users/newuser/Desktop/Notes/Graduate/STAT 408 - Applied Regression Analysis/seatpos.csv")
```

### Problem 2a

```{r Problem 2a}
summary(lm(hipcenter~.,data=s))
```

We can see the intercept term has by far the strongest magnitude in the model (`r summary(lm(hipcenter~.,data=s))$coefficients["(Intercept)","Estimate"]`), which balances out the variables with negative coefficients (`HtShoes`, `Arm`, `Thigh`, and `Leg`). The remaining four variables have relatively weak positive coefficients.

We can see from problem 2b that there are several variables that are very highly correlated with one another. It is likely that linear regression is not appropriate to use on these data.

### Problem 2b

```{r Problem 2b}
round(cor(s),3)
```

We can see the `HtShoes`, `Ht`, `Seated`, and `Leg` variables are all very highly correlated with one another ($r>$ `r sort(abs(cor(s[,c(3:5,8)])))[2*1+1]`) *except* for the pairing between the `Seated` and `Leg` variables which are strongly correlated with each other ($r=$ `r cor(s$Seated,s$Leg)`). The `Weight` variable is also strongly correlated with the `HtShoes` ($r=$ `r cor(s$Weight,s$HtShoes)`) and `Ht` ($r=$ `r cor(s$Weight,s$Ht)`) variables.

Looking at the model in problem 2a, there do not appear to be any apparent relations specifically between the high correlations and the model fitting. We can see the coefficients for the four variables have different signs. However, having too many variables highly correlated with one another likely produces misleading results for interpretation, and it is possible the effects of these variables may be negating each other or "cancelling each other out" in the model (Lecture 17, Slide 33).

### Problem 2c

```{r Problem 2c}
round(summary(prcomp(s[,-9],scale=TRUE))$importance,4) # Removing response variable (hipcenter)
sum(summary(prcomp(s[,-9],scale=TRUE))$importance["Standard deviation",1:2])
summary(prcomp(s[,-9],scale=TRUE))$importance["Cumulative Proportion","PC2"]
```

We can see the first two components have approximately `r 100*summary(prcomp(s[,-9],scale=TRUE))$importance["Cumulative Proportion","PC2"]` percent of the variance.

### Problem 2d

```{r Problem 2d}
prcomp(s[,-9],scale=TRUE)$rotation[,1:2] # Removing response variable (hipcenter)
```

We can see the first principal component is a linear combination of the variables as the signs of the coefficients are all the same. The second principal component appears to compare the `HtShoes`, `Ht`, `Seated`, and `Leg` variables (which we saw in problem 2b are very highly correlated with one another) with the other variables.

### Problem 2e

```{r Problem 2e}
spc<-data.frame(s$hipcenter,prcomp(s[,-9],scale=TRUE)$x[,1:2]) # Creating new dataframe
summary(lm(s.hipcenter~.,data=spc))
```

We can see the first principal component has a positive coefficient (`r summary(lm(s.hipcenter~.,data=spc))$coefficients["PC1","Estimate"]`) which suggests that all independent variables grouped together are directly proportional to the response variable `hipcenter`. The second principal component also has a positive coefficient (`r summary(lm(s.hipcenter~.,data=spc))$coefficients["PC2","Estimate"]`) which suggests that the group of highly correlated variables (`HtShoes`, `Ht`, `Seated`, and `Leg`) together are directly proportional to the response variable `hipcenter`.

Looking at the model in problem 2a, we can see the signs of the intercept terms are reversed. The intercept term in the principal component model is the same as the mean response (`r mean(s$hipcenter)`), which means the baseline observation $\beta_0$ has the same value for the response variable. Meanwhile, the intercept term in the linear model (`r summary(lm(hipcenter~.,data=s))$coefficients["(Intercept)","Estimate"]`) is much further away, making extrapolation more impractical and unreasonable. We can also see that the intercept term and all of the variables in the principal components model significant at the $\alpha=0.05$ level, while only the intercept term in the linear model is significant.

## Problem 3

```{r Problem 3}
f<-read.csv("/Users/newuser/Desktop/Notes/Graduate/STAT 408 - Applied Regression Analysis/fat.csv")
testf<-f[seq(1,nrow(f),10),]
trainf<-f[-seq(1,nrow(f),10),]
```

### Problem 3a

```{r Problem 3a}
summary(lm(siri~.-brozek-density,data=trainf)) # Removing brozek and density
LinearModel<-sqrt(mean((testf$siri-predict(lm(siri~.-brozek-density,data=trainf),testf))^2))
```

### Problem 3b

```{r Problem 3b}
step(lm(siri~.-brozek-density,data=trainf),trace=0,direction="backward") # Removing brozek and density
sw<-lm(siri~weight+adipos+free+chest+abdom+thigh+ankle+biceps+forearm,data=trainf)
summary(sw)
StepwiseAIC<-sqrt(mean((testf$siri-predict(sw,testf))^2))
```

### Problem 3c

```{r Problem 3c}
pc<-prcomp(f[,-c(1:3)],scale=TRUE) # Removing brozek, density, and response variable (siri)
round(summary(pc)$importance[,1:7],5)
trainpc<-data.frame(trainf$siri,pc$x[-seq(1,nrow(f),10),1:7]) # Creating new dataframes
testpc<-data.frame(pc$x[seq(1,nrow(f),10),1:7])
summary(lm(trainf.siri~.,data=trainpc))
PrincipalComponent<-sqrt(mean((testf$siri-predict(lm(trainf.siri~.,data=trainpc),testpc))^2))
```

### Problem 3d

```{r Problem 3d}
library(MASS)
r<-lm.ridge(siri~.-brozek-density,data=trainf,lambda=seq(0,0.05,0.002)) # Removing brozek and density
which.min(r$GCV) # Cross-validation for smallest tuning parameter
lr<-coef(r)[names(which.min(r$GCV)),]
lr
Ridge<-sqrt(mean((testf$siri-cbind(1,as.matrix(testf[,-c(1:3)]))%*%lr)^2)) # coding 13.R
```

### Problem 3e

```{r Problem 3e, message=FALSE}
library(lars)
lars<-lars(as.matrix(trainf[,-c(1:3)]),trainf$siri) # Removing brozek, density, and siri
set.seed(1112)                                      # coding 13.R
cv.lars(as.matrix(trainf[,-c(1:3)]),trainf$siri)
cv<-cv.lars(as.matrix(trainf[,-c(1:3)]),trainf$siri,plot.it=FALSE)
ll<-cv$index[which.min(cv$cv)]
points(ll,min(cv$cv),col="red",pch=16) # Cross-validation for smallest tuning parameter
legend("topright",col="red",pch=c(16,NA),legend=c("Index from cross-validation: 0.787879","Smallest tuning parameter: 3.573839"))
LASSO<-sqrt(mean((testf$siri-predict(lars,as.matrix(testf[,-c(1:3)]),s=ll,mode="fraction")$fit)^2))
```

### Problem 3f

```{r Problem 3f}
data.frame(LinearModel,StepwiseAIC,PrincipalComponent,Ridge,LASSO)
```

We can see the ridge regression model has the lowest root mean squared error (RMSE) at `r Ridge`. The full linear model (`r LinearModel`) and LASSO regression model (`r LASSO`) had marginally higher RMSE values, followed by the stepwise regression model (`r StepwiseAIC`). The principal component regression (PCR) model had the highest RMSE (`r PrincipalComponent`).

I am not too surprised by the comparison between the models. It makes sense intuitively that more complex methods like ridge and LASSO regression may be able to capture more complex trends in the data, and these methods also use cross-validation to choose the most optimal parameters. It also makes sense that the linear and stepwise regression models would perform slightly worse and similar to each other, as these are basic but robust methods. It may be possible that principal component regression is unsuitable for this dataset or that there are additional information in components 8-18 that were left out of the predictions, which may explain its relatively high RMSE.