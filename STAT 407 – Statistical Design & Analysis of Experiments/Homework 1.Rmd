---
title: "STAT 407 Homework 1"
author: "Charles Hwang"
date: "9/27/2021"
output: pdf_document
---
## Problem 2.15
(a) Yes, the null hypothesis can be rejected at the $\alpha$ = 0.05 level because the p-value = 0.001 < 0.05.

(b) This is a two-sided test. We can tell because the test says so ("T-Test of difference=0 (vs not = )").

(c) We would reject the null hypothesis at the $\alpha$ = 0.05 level under this null hypothesis because the 95% confidence interval does not include 2.

(d) We would reject the null hypothesis. We can answer this question without doing any additional calculations because it is the same null hypothesis as the one in Problem 2.15(c).

(e) The 95% upper confidence bound is -0.97135.

(f) $\bar{y}_1$ = 50.19, $s_1^2$ = 1.71, $n_1$ = 20, $\bar{y}_2$ = 52.52, $s_2^2$ = 2.48, $n_2$ = 20, $s_p$ = 2.1277, d = 2

t = $\frac{\bar{y}_1-\bar{y}_2-d}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}$ = $\frac{(50.19)-(52.52)}{(2.1277)\sqrt{\frac{1}{(20)}+\frac{1}{(20)}}}$ = -6.43542899306

```{r Problem 2.15(f)}
rm(list=ls())
2*pt(-6.43542899306,20+20-2) # P-value for two-sided t-test
```

## Problem 2.28
(a) $\bar{y}_1$ = 12.5, $s_1^2$ = 101.17, $n_1$ = 8, $\bar{y}_2$ = 10.2, $s_2^2$ = 94.73, $n_2$ = 9

Bartlett's test â€” T = 0.00773836595315706, p = 0.929902123688167

We fail to reject the null hypothesis at the $\alpha$ = 0.05 level. There is insufficient evidence (p = 0.929902123688) that the variances are unequal. Therefore, pooled variances should be used.

(b) t = $\frac{\bar{y}_2-\bar{y}_1}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}$, where $s_p$ = $\sqrt{\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}}$

$s_p$ = $\sqrt{\frac{((8)-1)(101.17)+((9)-1)(94.73)}{(8)+(9)-2}}$ = 9.8861182136

t = $\frac{(10.2)-(12.5)}{(9.8861182136)\sqrt{\frac{1}{(8)}+\frac{1}{(9)}}}$ = -0.47878862539

```{r Problem 2.28(b)}
pt(-0.47878862539,8+9-2)
```

We fail to reject the null hypothesis at the $\alpha$ = 0.05 level. There is insufficient evidence (p = .3194967) that the two means are different.

## Problem 2.31
(a)
```{r Problem 2.31(a)}
etch<-c(5.34,6.65,4.76,5.98,7.25,6.00,7.55,5.54,5.62,6.21,5.97,7.35,5.44,4.39,4.98,5.25,6.35,4.61,6.00,5.32)
df<-length(etch)-1
cat("(",(df)*var(etch)/qchisq(1-.05/2,df),", ",(df)*var(etch)/qchisq(.05/2,df),")",sep="")
```

(b)
```{r Problem 2.31(b)}
pchisq((df)*var(etch)/1,df) # Dividing by H_0: s^2 = 1
```

We fail to reject the null hypothesis at the $\alpha$ = 0.05 level. There is insufficient evidence (p = 0.2785805) that $\sigma\neq$ 1.

(c)
```{r Problem 2.31(c)}
shapiro.test(etch)
```

We assume the data are normal for the purposes of the chi-square test for variance. The sample size is 20, which is lower than the generally accepted value for assuming normality. Using the Shapiro-Wilk test for normality, the null hypothesis is not rejected, so the normality assumption appears to be met.

(d)
```{r Problem 2.31(d)}
qqnorm(etch)
qqline(etch)
```

There are few deviations in the Q-Q plot, so we can consider the data to be approximately normal.

## Problem 2.34
(a)
```{r Problem 2.34(a)}
k<-c(1.186,1.151,1.322,1.339,1.200,1.402,1.365,1.537,1.559)
l<-c(1.061,0.992,1.063,1.062,1.065,1.178,1.037,1.086,1.052)
t.test(k,l,paired=TRUE)
```

We reject the null hypothesis at the $\alpha$ = 0.05 level. There is sufficient evidence that there is a difference between the mean performance of the Karlsruhe method and the mean performance of the Lehigh method.

(b)
```{r Problem 2.34(b)}
t.test(k,l,paired=TRUE)$p.value
```

(c)
```{r Problem 2.34(c)}
t.test(k,l,paired=TRUE)$conf.int
```

(d)
```{r Problem 2.34(d)}
shapiro.test(k)
shapiro.test(l)
```

Neither of the null hypotheses were rejected, so the normality assumptions appear to be met. However, we should exercise some caution because the p-value of the Shapiro-Wilk test for normality for the Lehigh method is close to the $\alpha$ = 0.05 level.

(e)
```{r Problem 2.34(e)}
shapiro.test(k-l)
```

The null hypothesis was not rejected, so the normality assumption appears to be met for the difference in ratios between the mean performance of the Karlsruhe method and the mean performance of the Lehigh method.

(f) The normality assumption is part of the paired t-test to ensure the test is robust. If the data were not normal, outliers could have undue influence on the mean difference.

## Problem 3.7
(a) 
```{r Problem 3.7(a)}
library(car)
a<-c(3129,3000,2865,2890)
b<-c(3200,3300,2975,3150)
c<-c(2800,2900,2985,3050)
d<-c(2600,2700,2600,2765)
cement<-rbind(a,b,c,d)
strength<-c(cement)
tech<-as.factor(rep(c(1:4),4))
bartlett.test(strength~tech) # Testing normality and equal variance assumptions for ANOVA
leveneTest(strength~tech)
shapiro.test(a)
shapiro.test(b)
shapiro.test(c)
shapiro.test(d)
# None of the null hypotheses were rejected, so the normality and equal 
anova(lm(strength~tech))       # variance assumptions appear to be met.
```

We reject the null hypothesis at the $\alpha$ = 0.05 level. There is sufficient evidence (p = 0.0004887) that at least one of the means is different.

(b)
```{r Problem 3.7(b)}
stripchart(data.frame(t(cement)),xlab="Mixing Technique",ylab="Tensile Strength (lbs/in^2)",vertical=TRUE,xaxt="n")
axis(1,at=c(1:4))
```

It appears the tensile strengths of mixing technique 4 is different from the tensile strengths of mixing techniques 1 and 2, while the tensile strengths of mixing techniques 1, 2, and 3 are not significantly different from one another.

(c)
```{r Problem 3.7(c)}
library(agricolae)
LSD<-LSD.test(aov(strength~tech),"tech")
LSD
```

From the confidence intervals, it appears the mean tensile strength of mixing technique 4 is different from the mean tensile strengths of mixing techniques 1, 2, and 3.

(d)
```{r Problem 3.7(d)}
qqnorm(residuals(lm(strength~tech)))
qqline(residuals(lm(strength~tech)))
```

There are few deviations in the Q-Q plot, so we can consider the residuals to be approximately normal and the initial normality assumption to be valid.

(e)
```{r Problem 3.7(e)}
plot(predict(lm(strength~tech)),residuals(lm(strength~tech)))
abline(v=mean(strength))
```

All values either have high residuals or little to no residuals. We can see four distinct vertical sets corresponding to the four different mixing techniques.

(f)
```{r Problem 3.7(f)}
plot(as.numeric(tech),strength,xaxt="n")
axis(1,at=c(1:4))
```

I'm not sure exactly what scatter plot and results the problem is referring to, but this is similar to the stripchart.

## Problem 3.13
(a)
```{r Problem 3.13(a), warning=FALSE}
S<-c(3,5,3,7,6,5,3,2,1,6)
C<-c(1,3,4,7,5,6,3,2,1,7)
M<-c(4,1,3,5,7,1,2,4,2,7)
F<-c(3,5,7,5,10,3,4,7,2,7)
length<-c(rbind(S,C,M,F))
type<-rep(c("S","C","M","F"),10)
bartlett.test(length~type) # Testing normality and equal variance assumptions for ANOVA
leveneTest(length~type)
shapiro.test(S)
shapiro.test(C)
shapiro.test(M)
shapiro.test(F)
# None of the null hypotheses were rejected, so the normality and equal
anova(lm(length~type))         # variance assumptions appear to be met.
```

We fail to reject the null hypothesis at the $\alpha$ = 0.05 level. There is insufficient evidence (p = 0.3578) that at least one of the means is different.

(b)
```{r Problem 3.13(b)}
qqnorm(residuals(lm(length~type)))
qqline(residuals(lm(length~type)))
```

From the Q-Q plot of the residuals, it appears the model is fairly adequate.

(c) No, there should not be any concerns about the validity of the analysis of variance because it is still a numeric variable. The problem doesn't specify what exactly the observations in the data are, so we have no further information.

## Problem 3.16
(a)
```{r Problem 3.16(a)}
OO<-c(21.8,21.9,21.7,21.6,21.7)
ZS<-c(21.7,21.4,21.5,21.4,NA) # Adding missing values to equate lengths
SO<-c(21.9,21.8,21.8,21.6,21.5)
LS<-c(21.9,21.7,21.8,21.4,NA)
brick<-rbind(OO,ZS,SO,LS)
density<-brick[-c(18,20)] # Dropping missing values
temp<-as.factor(c(rep(c("OO","ZS","SO","LS"),4),"OO","SO"))
bartlett.test(density~temp) # Testing normality and equal variance assumptions for ANOVA
leveneTest(density~temp)
shapiro.test(OO)
shapiro.test(ZS)
shapiro.test(SO)
shapiro.test(LS)
# None of the null hypotheses were rejected, so the normality and equal
anova(lm(density~temp))        # variance assumptions appear to be met.
```

We fail to reject the null hypothesis at the $\alpha$ = 0.05 level. There is insufficient evidence (p = 0.1569) that at least one of the means is different.

(b) It is not appropriate to compare the means using Fisher's least significant difference in this experiment because we concluded in Problem 3.16(a) that none of the means were significantly different from one another.

(c)
```{r Problem 3.16(c)}
qqnorm(residuals(lm(density~temp)))
qqline(residuals(lm(density~temp)))
```

The residuals are normal and the analysis of variance assumptions are satisfied.

(d)
```{r Problem 3.16(d)}
stripchart(data.frame(t(brick)),xlab="Temperature",ylab="Density",vertical=TRUE,xaxt='n')
axis(1,at=c(1:4),labels=c(100,125,150,175)) # Changing labels for x-axis
```

The graph adequately summarizes the results of the analysis of variance in Problem 3.16(a). We can clearly see that none of the means are different from one other.

## Problem 3.27
(a)
```{r Problem 3.27(a)}
x<-c(58.2,57.2,58.4,55.8,54.9)
y<-c(56.3,54.5,57.0,55.3,NA) # Adding missing values to equate lengths
z<-c(50.1,54.2,55.4,NA,NA)
w<-c(52.9,49.9,50.0,51.7,NA)
mix<-rbind(x,y,z,w)
conc<-mix[-c(15,18:20)] # Dropping missing values
catalyst<-as.factor(c(rep(c("x","y","z","w"),3),"x","y","w","x"))
bartlett.test(conc~catalyst) # Testing normality and equal variance assumptions for ANOVA
leveneTest(conc~catalyst)
shapiro.test(x)
shapiro.test(y)
shapiro.test(z)
shapiro.test(w)
# None of the null hypotheses were rejected, so the normality and equal
anova(lm(conc~catalyst))       # variance assumptions appear to be met.
```

We reject the null hypothesis at the $\alpha$ = 0.05 level. There is sufficient evidence (p = 0.001436) that at least one of the means is different.

(b)
```{r Problem 3.27(b)}
qqnorm(residuals(lm(conc~catalyst)))
qqline(residuals(lm(conc~catalyst)))
```

There are few deviations in the Q-Q plot, so we can consider the data to be approximately normal and the analysis of variance assumptions to be satisfied.

(c)
```{r Problem 3.27(c)}
shapiro.test(x) # Testing normality assumption for one-sample t-interval
# The null hypothesis was not rejected, so the normality assumption appears to be met.
t.test(x,conf.level=0.99)$conf.int
```