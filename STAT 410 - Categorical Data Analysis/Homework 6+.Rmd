---
title: "Homework 6+"
author: "Charles Hwang"
date: "4/29/2022"
output: pdf_document
---
Charles Hwang

Dr. Whalen

STAT 410-001

29 April 2022

## Problem 6.2

```{r Problem 6.2, message=FALSE}
rm(list=ls())
all<-read.table("/Users/newuser/Desktop/Notes/Graduate/STAT 410 - Categorical Data Analysis/Alligators.dat",header=TRUE)
all$y<-as.factor(all$y)
library(VGAM)
alll<-vglm(y~x,family=multinomial,data=all)
summary(alll)
exp(alll@coefficients["x:1"])+exp(alll@coefficients["x:2"])
```

## Problem 6.4

```{r Problem 6.4}
al<-read.table("/Users/newuser/Desktop/Notes/Graduate/STAT 410 - Categorical Data Analysis/Afterlife.dat",header=TRUE)
al$race<-as.factor(al$race)
al$gender<-as.factor(al$gender)
algr<-vglm(cbind(yes,undecided,no)~gender+race,family=multinomial,data=al)
summary(algr)
algru<-vglm(cbind(yes,undecided,no)~gender+race,family=multinomial(refLevel="undecided"),data=al)
summary(algru)
exp(algr@coefficients["gendermale:2"])
exp(algru@coefficients["gendermale:1"])
```

(**a**) The estimated conditional odds ratio between gender and "undecided" vs. "no" belief in an afterlife ($\beta_2^G$), given race, is `r exp(algr@coefficients["gendermale:2"])`. The estimated the odds of an "undecided" response vs. a "no" response on belief in an afterlife for males are approximately `r exp(algr@coefficients["gendermale:2"])` times the same estimated odds for females, adjusting for race.

(**b**) The estimated conditional odds ratio between gender and affirmative ("yes") vs. "undecided" belief in an afterlife ($\beta_1^G$), given race, is `r exp(algru@coefficients["gendermale:1"])`. The estimated the odds of an affirmative ("yes") response vs. an "undecided" response on belief in an afterlife for males are approximately `r exp(algru@coefficients["gendermale:1"])` times the same estimated odds for females, adjusting for race.

## Problem 6.5

### Problem 6.5a

(i) more satisfied; (ii) less satisfied; (iii) less satisfied. This makes sense intuitively when looking at the equation, variables, and levels.

### Problem 6.5b

We can clearly see that $Y$ is maximized when $x_1=4$, $x_2=1$, and $x_3=1$. This also makes sense intuitively when looking at the equation, variables, and levels.

## Problem 6.6

We can see from Table 6.6 (page 188) that the estimated odds of being "very" happy vs. "not" happy for those with "above average" or "average" incomes are approximately $e^{-0.22751}\approx$ `r exp(-0.22751)` times the same estimated odds for those in the next-lowest income category.

We can also see that the estimated odds of being "pretty" happy vs. "not" happy for those with "above average" or "average" incomes are approximately $e^{-0.09615}\approx$ `r exp(-0.09615)` times the same estimated odds for those in the next-lowest income category.

We can see from the fitted values that the range of the outcome variable increase with income. Among those who are "not" happy, those with "below average" income ($[1,y1]$) tended to be more happy than those with "above average" income ($[1,y3]$), with "average" income ($[1,y2]$) between the two. Among those who are "very" happy, those with "above average" income ($[3,y3]$) tended to be more happy than those with "below average" income ($[3,y1]$), with "average" income ($[3,y2]$) between the two. This may indicate that a higher income can be polarizing for happiness, whether positive or negative.

We can see the income variable itself is not very significant in the model ($p=0.504907$, $p=0.430694$), indicating there are other variables that may be important, as expected.

In conclusion, marital happiness may be associated with family income, but there are clearly other variables not in the model that are more closely associated or interact with family income. Additional analysis is needed to form a definitive conclusion.

## Problem 6.7

### Problem 6.7a

The cumulative logit model is able to account for ordinal variables, like we have in the happiness and income categories. Section 6.2 (page 167) of the textbook writes: "When response categories are ordered, logits can utilize the ordering. This results in models that have fewer parameters and potentially greater power and simpler interpretation than baseline-category logit models."

### Problem 6.7b

```{r Problem 6.7b}
1-pchisq(3.2472,3)
```

The model fits adequately. With a residual deviance of 3.2472 on 3 degrees of freedom, we can see $H_0$ for the $\chi^2$ goodness-of-fit test is not rejected at the $\alpha=0.05$ level ($p=\chi^2_{3}(3.2472)=$ `r 1-pchisq(3.2472,3)`). There are $c=3$ response categories, which means there are $c-1=3-1=2$ intercepts. There is only one income effect because it is the same for each cumulative probability level.

### Problem 6.7c

```{r Problem 6.7c}
1-pchisq(4.13476-3.2472,4-3)
```

We can see from Tables 6.6 and 6.7 that the $\chi^2$ test statistic is $4.13476-3.2472=0.88756$ with $4-3=1$ degree of freedom and the *p*-value is `r 1-pchisq(4.13476-3.2472,4-3)`. We fail to reject $H_0$ at the $\alpha=0.05$ level and there is insufficient evidence that there is an association between income and happiness. The estimate for the income effect is $\beta_I=-0.1117$, which indicates the odds of being less happy decreases as income increases.

## Problem 8.1

```{r Problem 8.1}
mcnemar.test(matrix(c(159,8,22,14),nrow=2),correct=FALSE)
```

We reject $H_0$ at the $\alpha=0.05$ level. There is sufficient evidence ($\chi^2=$ `r mcnemar.test(matrix(c(159,8,22,14),nrow=2),correct=FALSE)$statistic`, $p=$ `r mcnemar.test(matrix(c(159,8,22,14),nrow=2),correct=FALSE)$p.value`) that there is a relationship between smoking and birth weight.

## Problem 8.3

```{r Problem 8.3}
log(359/785/(334/810))
log(132/107)
```

The $\hat{\beta}$ variable in (8.2) is for the entire sample, while the $\hat{\beta}$ variable in (8.3) is conditional on subject *i*. We can see that $\hat{\beta}$ from (8.2) is $\ln(\frac{\frac{n_{1\Sigma}}{n_{2\Sigma}}}{\frac{n_{\Sigma1}}{n_{\Sigma2}}})=\ln(\frac{\frac{359}{785}}{\frac{334}{810}})=\ln(\frac{29079}{26219})\approx$ `r log(359/785/(334/810))`, while $\hat{\beta}$ from (8.3) is $\ln(\frac{n_{12}}{n_{21}})=\ln(\frac{132}{107})\approx$ `r log(132/107)`.

## Problem 8.4

We estimate that $e^{\hat{\beta}}=\frac{16}{37}\approx$ `r 16/37`. This estimate is valid because it is conditional on the subject (subject-specific), taken from (8.3).

## Problem 8.6

McNemar's test is used when there are two binary factor variables and a two-way table is created, while the paired-difference *t*-test is used when the variables are numeric and normally distributed.

## Problem 9.1

### Problem 9.1(a)

$logit[P(Y_t=1)]=\alpha+\beta_Az_A+\beta_Cz_C+\beta_Mz_M+\gamma x$, where $\beta_A$, $\beta_C$, and $\beta_M$ are the coefficients for alcohol, cigarette, and marijuana and $z_A$, $z_C$, and $z_M$ are binary variables for the usage of each respectively (yes = 1, no = 0). The hypotheses for testing for marginal homogeneity would be:

$H_0:\beta_A=\beta_C=\beta_M$

$H_A:$ At least one $\beta_i$ is different

### Problem 9.1(b)

$logit[P(Y_{it}=1)]=\alpha_i+\beta_Az_A+\beta_Cz_C+\beta_Mz_M+\gamma_ix$

The interpretations for the $\beta_i$'s and $z_i$'s would generally be the same, but we can see the intercept and error terms $\alpha_i$ and $\gamma_i$ now incorporate individual subjects *i*.

## Problem 9.3

### Problem 9.3(a)

$logit(\hat{\pi})=-0.57+1.93(0)+0.86(0)+0.38r-0.20g+0.37g(0)+0.22g(0)$; where $s_1=0$, $s_2=0$

$logit(\hat{\pi})=-0.57+0.38r-0.20g$

We can clearly see this equation is maximized when $r=1$ (white) and $g=0$ (male).

$logit(\hat{\pi})=-0.57+1.93(1)+0.86(0)+0.38r-0.20g+0.37g(1)+0.22g(0)$; where $s_1=1$, $s_2=0$

$logit(\hat{\pi})=-0.57+1.93+0.38r-0.20g+0.37g$

$logit(\hat{\pi})=1.36+0.38r+0.17g$

White females have the highest estimated probability of use of **alcohol**. We can clearly see this equation is maximized when $r=1$ (white) and $g=1$ (female).

$logit(\hat{\pi})=-0.57+1.93(0)+0.86(1)+0.38r-0.20g+0.37g(0)+0.22g(1)$; where $s_1=0$, $s_2=1$

$logit(\hat{\pi})=-0.57+0.86+0.38r-0.20g+0.22g$

$logit(\hat{\pi})=0.29+0.38r+0.02g$

White females have the highest estimated probability of use of **cigarettes**. We can clearly see this equation is maximized when $r=1$ (white) and $g=1$ (female).

### Problem 9.3(b)

$logit(\hat{\pi})=-0.57+1.93s_1+0.86s_2+0.38r-0.20g+0.37gs_1+0.22gs_2$ (original equation)

We can see that $e^{\beta_r}=e^{0.38}\approx$ `r exp(0.38)`.

### Problem 9.3(c)

$logit(\hat{\pi})=1.36+0.38r+0.17g$; where $s_1=1$, $s_2=0$ (from Problem 9.3(a))

We can see that $e^{\beta_g}=e^{0.17}\approx$ `r exp(0.17)`.

$logit(\hat{\pi})=0.29+0.38r+0.02g$; where $s_1=0$, $s_2=1$ (from Problem 9.3(a))

We can see that $e^{\beta_g}=e^{0.02}\approx$ `r exp(0.02)`.

$logit(\hat{\pi})=-0.57+0.38r-0.20g$; where $s_1=0$, $s_2=0$ (from Problem 9.3(a))

We can see that $e^{\beta_g}=e^{-0.20}\approx$ `r exp(-0.2)`.

### Problem 9.3(d)

$logit(\hat{\pi})=-0.57+1.93s_1+0.86s_2+0.38r-0.20(1)+0.37(1)s_1+0.22(1)s_2$; where $g=1$

$logit(\hat{\pi})=-0.77+2.30s_1+1.08s_2+0.38r$

We can see that $e^{\beta_{s_1}}=e^{2.30}\approx$ `r exp(2.30)` and that $e^{\beta_{s_2}}=e^{1.08}\approx$ `r exp(1.08)`.

### Problem 9.3(e)

$logit(\hat{\pi})=-0.57+1.93s_1+0.86s_2+0.38r-0.20(0)+0.37(0)s_1+0.22(0)s_2$; where $g=0$

$logit(\hat{\pi})=-0.57+1.93s_1+0.86s_2+0.38r$

We can see that $e^{\beta_{s_1}}=e^{1.93}\approx$ `r exp(1.93)` and that $e^{\beta_{s_2}}=e^{0.86}\approx$ `r exp(0.86)`.

## Problem 9.4

### Problem 9.4(a)

$logit(\hat{\pi})=-0.02810-1.31391s-0.05927(1)+0.048246t+1.01719(1)t$; where $d=1$

$logit(\hat{\pi})=-0.02810-1.31391s-0.05927+0.048246t+1.01719t$

$logit(\hat{\pi})=-0.08737-1.31391s+1.065436t$

We can see that $e^{\beta_t}=e^{1.065436}\approx$ `r exp(1.065436)`. The estimated odds of a normal classification of a subject's level of mental depression after *t* weeks of using the new drug are approximately $e^{1.065436}\approx$ `r exp(1.065436)` times the same estimated odds after $\frac{t}{2}$ weeks of using the new drug, where $t=1,2,$ or 4, holding initial severity constant.

$logit(\hat{\pi})=-0.02810-1.31391s-0.05927(0)+0.048246t+1.01719(0)t$; where $d=0$

$logit(\hat{\pi})=-0.02810-1.31391s+0.048246t$

We can see that $e^{\beta_t}=e^{0.048246}\approx$ `r exp(0.048246)`. The estimated odds of a normal classification of a subject's level of mental depression after *t* weeks of using the standard drug are approximately $e^{0.048246}\approx$ `r exp(0.048246)` times the same estimated odds after $\frac{t}{2}$ weeks of using the standard drug, where $t=1,2,$ or 4, holding initial severity constant.

### Problem 9.4(b)

$logit(\hat{\pi})=-0.02810-1.31391s-0.05927d+0.048246t+1.01719dt$ (original equation)

We can see when using the coefficients with drug (*d*) that $e^{\beta_d+\beta_{dt}t}=e^{-0.05927+1.01719t}$. The estimated odds of a normal classification of a subject's level of mental depression after *t* weeks of using the new drug are approximately $e^{-0.05927+1.01719\log_2t}$ times the same estimated odds after *t* weeks of using the standard drug, where $t=1,2,$ or 4, holding initial severity constant.

## Problem 9.11

### Problem 9.11(a)

```{r Problem 9.11(a), message=FALSE}
i<-read.table("/Users/newuser/Desktop/Notes/Graduate/STAT 410 - Categorical Data Analysis/Insomnia2.dat",header=TRUE)
library(VGAM)
names(i)<-c("Treatment","Initial","F10","F25","F45","F75")
tm<-vglm(cbind(F10,F25,F45,F75)~Treatment*Initial,family=cumulative(parallel=TRUE),data=i)
summary(tm)
```

$logit[P(Y_2\leq j)]=\alpha_j-0.217123x-0.052835y_1+0.021737xy_1$ (original equation)

$logit[P(Y_2\leq j)]=\alpha_j-0.217123x-0.052835(10)+0.021737x(10)$; where $y_1=10$

$logit[P(Y_2\leq j)]=\alpha_j-0.217123x-0.52835+0.21737x$

$logit[P(Y_2\leq j)]=\alpha_j-0.52835+0.000247x$

We can see the estimated treatment effect at $y_1=10$ is approximately $\hat{\beta}_2=0.000247$.

$logit[P(Y_2\leq j)]=\alpha_j-0.217123x-0.052835(25)+0.021737x(25)$; where $y_1=25$

$logit[P(Y_2\leq j)]=\alpha_j-0.217123x-1.320875+0.543425x$

$logit[P(Y_2\leq j)]=\alpha_j-1.320875+0.326302x$

We can see the estimated treatment effect at $y_1=25$ is approximately $\hat{\beta}_2=0.326302$.

$logit[P(Y_2\leq j)]=\alpha_j-0.217123x-0.052835(45)+0.021737x(45)$; where $y_1=45$

$logit[P(Y_2\leq j)]=\alpha_j-0.217123x-2.377575+0.978165x$

$logit[P(Y_2\leq j)]=\alpha_j-2.377575+0.761042x$

We can see the estimated treatment effect at $y_1=45$ is approximately $\hat{\beta}_2=0.761042$.

$logit[P(Y_2\leq j)]=\alpha_j-0.217123x-0.052835(75)+0.021737x(75)$; where $y_1=75$

$logit[P(Y_2\leq j)]=\alpha_j-0.217123x-3.962625+1.630275x$

$logit[P(Y_2\leq j)]=\alpha_j-3.962625+1.413152x$

We can see the estimated treatment effect at $y_1=75$ is approximately $\hat{\beta}_2=1.413152$.

### Problem 9.11(b)

```{r Problem 9.11(b)}
i$Initial<-as.factor(i$Initial)
q<-vglm(cbind(F10,F25,F45,F75)~Treatment+Initial,family=cumulative(parallel=TRUE),data=i)
summary(q) # (i)
x<-vglm(cbind(F10,F25,F45,F75)~Treatment*Initial,family=cumulative(parallel=TRUE),data=i)
summary(x) # (ii)
```

(**i**) We can see the estimated treatment log odds ratio is approximately $\ln(e^{\beta_x})=\beta_x=0.9108$, assuming no interaction. The estimated "log" odds of a patient receiving the active drug falling asleep after *n* minutes are approximately 0.9108 times the estimated "log" odds of a patient receiving the placebo falling asleep after *n* minutes, where $n=10,25,45,$ or 75 and no interaction is assumed, holding all other variables constant.

(**ii**) We can see the estimated treatment log odds ratio is approximately $\ln(e^{\beta_x})=\beta_x=0.5284$ when allowing for interaction. The estimated "log" odds of a patient receiving the active drug falling asleep after *n* minutes are approximately 0.5284 times the estimated "log" odds of a patient receiving the placebo falling asleep after *n* minutes, where $n=10,25,45,$ or 75 and interaction is assumed, holding all other variables constant.

## Problem 10.3

### Problem 10.3(a)

Sample proportion estimates: $\frac{2}{5},\frac{4}{5},\frac{1}{5},\frac{3}{5},\frac{3}{5},\frac{5}{5},\frac{4}{5},\frac{2}{5},\frac{3}{5},\frac{1}{5}$

Mode: $logit(\pi_i)=u_i+\alpha$

### Problem 10.3(b)

```{r Problem 10.3(b), message=FALSE}
library(lme4)
h<-c(2,4,1,3,3,5,4,2,3,1)
f<-rep(5,10)
c<-1:10
rem<-glmer(h/f~(1|c),family=binomial,weights=f,nAGQ=100)
summary(rem)
exp(rem@beta)/(1+exp(rem@beta))
rem@beta
rem@theta
round(fitted(rem),5)
```

We can see the maximum-likelihood estimates are $\hat{\pi}_i=$ `r exp(rem@beta)/(1+exp(rem@beta))`, $\hat{\alpha}=$ `r rem@beta`, and $\hat{\sigma}=$ `r rem@theta`. We can also see the predicted values of the probability of a head for each of the ten coins.

## Problem 10.4

### Problem 10.4(a)

```{r Problem 10.4(a), message=FALSE}
sub<-read.table("/Users/newuser/Desktop/Notes/Graduate/STAT 410 - Categorical Data Analysis/Substance.dat",header=TRUE)
library(dplyr)
library(tibble)
library(tidyr)
s<-sub %>%
uncount(count) %>%
rowid_to_column(var="ID") %>%
pivot_longer(cols=-ID,names_to="Drug",values_to="u") %>%
mutate(Drug=factor(Drug,levels=c("cigarettes","alcohol","marijuana"),labels=c(1,2,3)),u=ifelse(u=="yes",1,0))
m<-glmer(u~(1|ID)+Drug,family=binomial,nAGQ=100,data=s)
summary(m)
library(geepack)
anova(geeglm(u~Drug,id=ID,family="binomial",corstr="exchangeable",data=s))
```

We can see that $\hat{\beta}_A=$ `r m@beta[2]` and $\hat{\beta}_M=$ `r m@beta[3]`.

We reject $H_0$ at the $\alpha=0.05$ level. There is strong evidence ($p=\chi_2($ `r anova(geeglm(u~Drug,id=ID,family="binomial",corstr="exchangeable",data=s))$X2` ) < 0.000000000000001) that at least one $\beta_i$ is different.

### Problem 10.4(b)

We can see that $\hat{\sigma}=$ `r m@theta`.

(**i**) The large value implies that the probability of an affirmative response has a high variance as ID changes.

(**ii**) A large positive value for $u_i$ for a particular student implies they "start out" with a high probability of an affirmative response prior to taking variables into account.

### Problem 10.4(c)

```{r Problem 10.4(c)}
library(gee)
gee(u~Drug,id=ID,family=binomial(link="logit"),data=s)
```

We can see the $\{\beta_t\}$ for the GLMM are $\hat{\beta}_A=$ `r m@beta[2]` and $\hat{\beta}_M=$ `r m@beta[3]` and the $\{\beta_t\}$ for the GEE are $\hat{\beta}_A=$ `r gee(u~Drug,id=ID,family=binomial(link="logit"),data=s)$coefficients["Drug2"]` and $\hat{\beta}_M=$ `r gee(u~Drug,id=ID,family=binomial(link="logit"),data=s)$coefficients["Drug3"]`. There are notable differences in method and approach between the two models that would lead them to produce different parameter estimates (Section 10.2.5, pages 283-284).

### Problem 10.4(d)

```{r Problem 10.4(d)}
summary(glm(count~alcohol*cigarettes+alcohol*marijuana+cigarettes*marijuana,family=poisson,data=sub))
```

We can see the random effects and marginal models take a binary response variable and use a logit while the loglinear model uses count data and the Poisson distribution. The focus for the random effects and marginal models appears to be on the independent variable (drug) since it has multiple levels, while the focus in the loglinear model appears to be on the dependent count variable.

### Problem 10.4(e)

```{r Problem 10.4(e)}
s2<-read.table("/Users/newuser/Desktop/Notes/Graduate/STAT 410 - Categorical Data Analysis/Substance2.dat",header=TRUE)
s2<-s2 %>%
uncount(count) %>%
rowid_to_column(var="ID") %>%
pivot_longer(cols=-c(ID,R,G),names_to="Drug",values_to="Use") %>% # Excluding "R" and "G"
mutate(Drug=factor(Drug,levels=c("C","A","M"),labels=c(1,2,3)),Use=ifelse(Use==1,1,0))
mrg<-glmer(Use~(1|ID)+factor(Drug,c(3,1,2))*G+R,family=binomial,nAGQ=100,data=s2)
summary(mrg)
mrg@beta
mrg@theta
```

(**i**) We can see from the GLMM that $\hat{\alpha}=$ `r mrg@beta[1]`, $\hat{\beta}_C=$ `r mrg@beta[2]`, $\hat{\beta}_A=$ `r mrg@beta[3]`, $\hat{\beta}_r=$ `r mrg@beta[4]`, $\hat{\beta}_g=$ `r mrg@beta[5]`, $\hat{\beta}_{gC}=$ `r mrg@beta[6]`, $\hat{\beta}_{gA}=$ `r mrg@beta[7]`, and $\hat{\sigma}=$ `r mrg@theta`.

(**ii**) We can see that the two models are different. The signs for the $\hat{\beta}_r$'s and $\hat{\beta}_g$'s are different between the models which illustrates the difference between the two methods.

## Problem 10.6

```{r Problem 10.6}
d<-read.table("/Users/newuser/Desktop/Notes/Graduate/STAT 410 - Categorical Data Analysis/Depression.dat",header=TRUE)
mD<-glmer(outcome~(1|case)+severity+drug*time,family=binomial,nAGQ=100,data=d)
summary(mD)
mD@beta
```

The estimated odds of a subject with initial severe depression having their level of mental depression being classified as normal after *t* weeks are approximately `r exp(mD@beta[2])` times the same estimated odds for a subject with initial mild depression, holding all other variables constant.

The estimated odds of a normal classification of a subject's level of mental depression after *t* weeks of using the new drug are approximately $e^{-0.05969884+1.01841530\log_2t}$ times the same estimated odds after *t* weeks of using the standard drug, where $t=1,2,$ or 4, holding all other variables constant. (From Problem 9.4(b))

We can see when using the coefficients with time that $e^{\beta_t+\beta_{dt}d}=e^{-0.05969884+1.01841530d}$. The estimated odds of a normal classification of a subject's level of mental depression after *t* weeks of treatment are approximately $e^{-0.05969884+1.01841530d}$ times the same estimated odds after $\frac{t}{2}$ weeks of treatment, where $t=1,2,$ or 4, holding all other variables constant.

We can see the interaction term is present if $d=1$ and $t=1$ or 2. The estimated odds of a normal classification of a subject's level of mental depression after 4 weeks of using the new drug are approximately `r exp(mD@beta[5])` times the same estimated odds after 2 weeks of using the new drug, holding all other variables constant. Additionally, the estimated odds of a normal classification of a subject's level of mental depression after 2 weeks of using the new drug are approximately `r exp(mD@beta[5])` times the same estimated odds after *either* 1 week of using the new drug *or any* duration using the standard drug, holding all other variables constant.

We can see this model is almost identical to the marginal model.