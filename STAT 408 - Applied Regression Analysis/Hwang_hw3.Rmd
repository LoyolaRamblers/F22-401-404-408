---
title: "Homework 3"
author: "Charles Hwang"
date: "10/13/2022"
output: pdf_document
---
Charles Hwang

Dr. Xi

STAT 408-001

2022 October 13

## Problem 1

```{r Problem 1}
rm(list=ls())
p<-read.csv("/Users/newuser/Desktop/Notes/Graduate/STAT 408 - Applied Regression Analysis/prostate.csv")
l<-summary(lm(lpsa~.,data=p))
t<-c()
```

### Problem 1a

```{r Problem 1a}
l$coefficients["age","Estimate"]+c(1,-1)*qnorm(0.05/2)*l$coefficients["age","Std. Error"]
```

We are **95** percent confident $\beta_{age}$ is between `r l$coefficients["age","Estimate"]+qnorm(0.05/2)*l$coefficients["age","Std. Error"]` and `r l$coefficients["age","Estimate"]-qnorm(0.05/2)*l$coefficients["age","Std. Error"]`.

### Problem 1b

```{r Problem 1b}
l$coefficients["age","Estimate"]+c(1,-1)*qnorm(0.1/2)*l$coefficients["age","Std. Error"]
```

We are **90** percent confident $\beta_{age}$ is between `r l$coefficients["age","Estimate"]+qnorm(0.1/2)*l$coefficients["age","Std. Error"]` and `r l$coefficients["age","Estimate"]-qnorm(0.1/2)*l$coefficients["age","Std. Error"]`.

### Problem 1c

```{r Problem 1c}
l$coefficients["age","Pr(>|t|)"]
```

Based on the confidence intervals found in problems 1a and 1b, we can expect $0.05<p<0.1$. This is consistent with the summary output showing $p=$ `r l$coefficients["age","Pr(>|t|)"]`.

### Problem 1d

```{r Problem 1d}
set.seed(1610)
for(i in 1:1000){r<-lm(lpsa~lcavol+lweight+sample(age)+lbph+svi+lcp+gleason+pgg45,data=p)
t[i]<-summary(r)$coefficients["sample(age)","t value"]}
l$coefficients["age","t value"]
hist(t,breaks=25)
abline(v=l$coefficients["age","t value"],col="red")
mean(abs(t)>abs(l$coefficients["age","t value"]))
```

The permutation $t$-test yields a $p$-value of `r mean(abs(t)>abs(l$coefficients["age","t value"]))`. This is very similar to $p$-value from the summary output printed in problem 1c (`r l$coefficients["age","Pr(>|t|)"]`).

### Problem 1e

```{r Problem 1e}
l$coefficients[,"Pr(>|t|)"]<0.05
anova(lm(lpsa~lcavol+lweight+svi,data=p),lm(lpsa~.,data=p))
```

We fail to reject $H_0$ at the $\alpha=0.05$ level. There is insufficient evidence ($F=$ `r anova(lm(lpsa~lcavol+lweight+svi,data=p),lm(lpsa~.,data=p))$F[2]`, $p=$ `r anova(lm(lpsa~lcavol+lweight+svi,data=p),lm(lpsa~.,data=p))$'Pr(>F)'[2]`) that the full model is better than the reduced model. We conclude the reduced model is better.

## Problem 2

```{r Problem 2}
c<-read.csv("/Users/newuser/Desktop/Notes/Graduate/STAT 408 - Applied Regression Analysis/cheddar.csv")
```

### Problem 2a

```{r Problem 2a}
summary(lm(taste~.,data=c))$coefficients[,"Pr(>|t|)"]
```

We can see the variables for both hydrogen sulfide ($p=$ `r summary(lm(taste~.,data=c))$coefficients["H2S","Pr(>|t|)"]`) and lactic acid ($p=$ `r summary(lm(taste~.,data=c))$coefficients["Lactic","Pr(>|t|)"]`) are statistically significant at the $\alpha=0.05$ level, but the variable for acetic acid is not ($p=$ `r summary(lm(taste~.,data=c))$coefficients["Acetic","Pr(>|t|)"]`).

### Problem 2b

```{r Problem 2b}
summary(lm(taste~exp(Acetic)+exp(H2S)+Lactic,data=c))$coefficients[,"Pr(>|t|)"]
```

We can see the variable for lactic acid is statistically significant ($p=$ `r summary(lm(taste~exp(Acetic)+exp(H2S)+Lactic,data=c))$coefficients["Lactic","Pr(>|t|)"]`) at the $\alpha=0.05$ level, but the variables for acetic acid ($p=$ `r summary(lm(taste~exp(Acetic)+exp(H2S)+Lactic,data=c))$coefficients["exp(Acetic)","Pr(>|t|)"]`) and hydrogen sulfide ($p=$ `r summary(lm(taste~exp(Acetic)+exp(H2S)+Lactic,data=c))$coefficients["exp(H2S)","Pr(>|t|)"]`) are not.

### Problem 2c

```{r Problem 2c}
anova(lm(taste~.,data=c),lm(taste~exp(Acetic)+exp(H2S)+Lactic,data=c))
par(mfrow=c(2,2))
hist(c$Acetic,breaks=10)
hist(exp(c$Acetic),breaks=10)
hist(c$H2S,breaks=10)
hist(exp(c$H2S),breaks=10)
```

We should not use an $F$-test to compare these two models as they have the same number of degrees of freedom and the difference is `r anova(lm(taste~.,data=c),lm(taste~exp(Acetic)+exp(H2S)+Lactic,data=c))$Df[2]` degrees of freedom, which is in the denominator of the $F$-statistic. There is also a negative sum of squares (`r anova(lm(taste~.,data=c),lm(taste~exp(Acetic)+exp(H2S)+Lactic,data=c))$'Sum of Sq'[2]`) which indicates something is clearly violated. However, we can plot histograms of the variables for hydrogen sulfide and lactic acid and their original scale, and we can see the logarithmically transformed versions of the variables are closer to normal. Thus, we can reasonably conclude the original model in problem 2a provides a better fit for these data.

### Problem 2d

```{r Problem 2d}
0.01*summary(lm(taste~.,data=c))$coefficients["H2S","Estimate"]
```

We would expect an increase in approximately `r 0.01*summary(lm(taste~.,data=c))$coefficients["H2S","Estimate"]` average taste score points if hydrogen sulfate were to be increased by 0.01 units, holding all other variables constant.

## Problem 3

```{r Problem 3}
g<-read.csv("/Users/newuser/Desktop/Notes/Graduate/STAT 408 - Applied Regression Analysis/teengamb.csv")
```

### Problem 3a

```{r Problem 3a}
summary(lm(gamble~.,data=g))$coefficients[,"Pr(>|t|)"]
```

We can see the variables for sex ($p=$ `r summary(lm(gamble~.,data=g))$coefficients["sex","Pr(>|t|)"]`) and income ($p=$ `r summary(lm(gamble~.,data=g))$coefficients["income","Pr(>|t|)"]`) are statistically significant at the $\alpha=0.05$ level, but the variables for socioeconomic status ($p=$ `r summary(lm(gamble~.,data=g))$coefficients["status","Pr(>|t|)"]`) and verbal score ($p=$ `r summary(lm(gamble~.,data=g))$coefficients["verbal","Pr(>|t|)"]`) are not.

### Problem 3b

Yes, the variables that the model found statistically significant make sense. It is reasonable to expect a difference in sex in amount of gambling, and intuitively, income should be a significant variable in the model.

### Problem 3c

```{r Problem 3c}
anova(lm(gamble~.,data=g),lm(gamble~income,data=g))
```

We reject $H_0$ at the $\alpha=0.05$ level. There is sufficient evidence ($F=$ `r anova(lm(gamble~.,data=g),lm(gamble~income,data=g))$F[2]`, $p=$ `r anova(lm(gamble~.,data=g),lm(gamble~income,data=g))$'Pr(>F)'[2]`) that the full model is better than the reduced model with only `income` as a predictor variable.

## Problem 4

```{r Problem 4}
s<-read.csv("/Users/newuser/Desktop/Notes/Graduate/STAT 408 - Applied Regression Analysis/sat.csv")
m<-lm(total~expend+salary+ratio+takers,data=s)
```

### Problem 4a

```{r Problem 4a}
plot(m$residuals~m$fitted.values)
abline(h=0)
```

There does not appear to be any pattern in the residuals vs. fitted values plot.

### Problem 4b

```{r Problem 4b}
qqnorm(m$residuals)
qqline(m$residuals)
```

There is slight tail at each of the ends of the Q-Q plot, but the normality assumption does not appear to be badly violated.

### Problem 4c

```{r Problem 4c}
rstudent(m)
sum(abs(rstudent(m))>qt(1-0.05/2,abs(diff(dim(s)))-1))
rstudent(m)[which(abs(rstudent(m))>qt(1-0.05/2,abs(diff(dim(s)))-1))]
```

We can see that observations `r which(abs(rstudent(m))>qt(1-0.05/2,abs(diff(dim(s)))-1))` appear to be statistical outliers. (Lecture 9, Slide 21)

### Problem 4d

```{r Problem 4d}
dfbeta(m)
library(tools)
ts<-toTitleCase(colnames(s))[c(1:4)]
par(mfrow=c(2,2))
for(j in 1:4){plot(dfbeta(m)[,j+1],main=substitute(paste("Change in ",x),list(x=ts[j])))
  e<-which(abs(dfbeta(m)[,j+1])>3*IQR(dfbeta(m)[,j+1]))
  points(e,dfbeta(m)[e,j+1],pch=19,col="red")
  abline(h=0)}
```

We can see that observation `r which(abs(dfbeta(m)[,"expend"])>3*IQR(dfbeta(m)[,"expend"]))` is an influential point for the expend variable; observations `r which(abs(dfbeta(m)[,"salary"])>3*IQR(dfbeta(m)[,"salary"]))` are influential points for the salary variable; observations `r which(abs(dfbeta(m)[,"ratio"])>3*IQR(dfbeta(m)[,"ratio"]))` are influential points for the ratio variable; and observation `r which(abs(dfbeta(m)[,"takers"])>3*IQR(dfbeta(m)[,"takers"]))` is an influential point for the takers variable.

## Problem 5

```{r Problem 5}
d<-read.csv("/Users/newuser/Desktop/Notes/Graduate/STAT 408 - Applied Regression Analysis/divusa.csv")
dm<-lm(divorce~unemployed+femlab+marriage+birth+military,data=d)
plot(dm$residuals~d$year,main="Residuals vs. Year",xlim=round(range(d$year),-1))
plot(tail(dm$residuals,nrow(d)-1)~head(dm$residuals,nrow(d)-1))
ds<-summary(lm(tail(dm$residuals,nrow(d)-1)~head(dm$residuals,nrow(d)-1)))
abline(ds$coefficients["(Intercept)","Estimate"],ds$coefficients["head(dm$residuals, nrow(d) - 1)","Estimate"])
ds
```

We can see from the first plot that the data are correlated by year as there is a clear sinusoidal pattern. The second plot shows no apparent pattern in the residuals of observation $t+1$ vs. the residuals of observation $t$.